---
title: "Analyze: LASSO Model"
output: html_notebook
---



<!-- NOTE: THIS ALL NEEDS TO BE REWORKED TO DEAL WITH THE FACT THAT A) IT WAS NOT ORIGINALLY SET UP CORRECTLY TO EVALUATE TEST AND B) THE FACT THAT THE TEST DATA DOES NOT ACTUALLY HAVE ANY LABELS.-->

```{r libs}

library(tidyverse)
library(broom)

library(Matrix)
library(glmnet)

```

```{r data}

test_final <- readRDS('../data/test_final.rds')
cvglmnet_results <- readRDS('../data/cvglmnet_results.rds')

```

```{r plot_cvglmnet_results}

plot(cvglmnet_results)

```


```{r train_prep}

test_x_prep <- test_final %>%
  # We remove the unique passenger ID from the X matrix, along with the Survived
  # label (will be used for y), and also the following variables because on
  # their own they are not good features for model training (we have already
  # generated other feats from them): Name, Ticket, Cabin.
  select(-PassengerId, -Name, -Ticket, -Cabin) %>% 
  # Interaction variables added here
  mutate(pclass_x_numfam = Pclass*num_family_members,
         age_x_fare = Age*Fare)

# Now with only X vars needed for model, convert to model matrix (so that
# categoricals get dummified).
test_x <- model.matrix(~.-1, test_x_prep)

test_y <- test_final %>% 
  select(Survived) %>% 
  pull()

```

```{r remove_feats_less_than_one_perc}

# Remove logical/binary feats with less > 99% uniformity in responses.

train_x_dummy_df <- as_tibble(train_x)

train_x_feat_summ <- train_x_dummy_df %>% 
  # Remove these vars consideration since they are numeric vars or are
  # categorical vars with > 2 responses.
  select(-Age, -Fare, -Parch, -SibSp, -num_family_members, -total_cabins, -pclass_x_numfam, -age_x_fare, -married_femaleYes, -married_femaleNotApplicable, -adultYes, -adultUNKNOWN) %>% 
  gather(feat, val) %>% 
  group_by(feat) %>% 
  mutate(feat_n = n()) %>% 
  ungroup() %>% 
  group_by(feat, val) %>% 
  mutate(feat_val_n = n()) %>% 
  ungroup() %>% 
  unique() %>% 
  mutate(coverage = feat_val_n/feat_n)

uniform_feat <- train_x_feat_summ %>% 
  #Identify those with > 99% uniformity
  filter(coverage > .99) %>% 
  select(feat) %>% 
  unique() %>% 
  pull()

train_x_mod <- train_x_dummy_df %>% 
  select(-one_of(uniform_feat))

# Convert back to matrix.
train_x <- model.matrix(~.-1, train_x_mod)

```

```{r best_fit}



best_lambda <- cvglmnet_results$lambda.min

best_fit <- glmnet(train_x,
                   train_y,
                   # Logistic model
                   family = "binomial",
                   # Use alpha = 1 for lasso.
                   alpha = 1,
                   lambda = best_lambda)

```

```{r predictions}

train_final$preds <- predict(cvglmnet_results, s = "lambda.min", newx = train_x, type = 'response')
```

```{r pred_labels}

train_final <- train_final %>% 
  mutate(pred_label = if_else(preds >= .5, 'yes', 'no'))

```

```{r compare_final}

compare_final <- train_final %>% 
  mutate(match = if_else(Survived == pred_label, TRUE, FALSE, FALSE))

# Overall accuracy
paste0(round((sum(compare_final$match)/nrow(compare_final))*100, 0), '%')

```

```{r}
coef(best_fit)
```




