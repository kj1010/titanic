---
title: "Build: LASSO Model"
output: html_notebook
---

```{r libs}

library(tidyverse)

library(Matrix)
library(glmnet)

```

```{r data}
train_final <- readRDS('../data/train_final.rds')
```

```{r train_prep}

# NOTE: There are 2 passengers in TRAIN whose Embarked variable was left blank
# (appear as ""). This will cause issues with the predictions on TEST, because
# there are no such passengers in TEST. Therefore, since we cannot predict on
# this value in a dummified matrix in TEST anyway, and it will lead to anchoring
# issues (the TRAIN will anchor to the "" value, and the TEST will anchor to the
# "C" value), we will simply remove these.
#train_x_prep %>% count(Embarked)
# A tibble: 4 x 2
#  Embarked     n
#  <chr>    <int>
#1 ""           2
#2 C          168
#3 Q           77
#4 S          644

train_x_prep <- train_final %>%
  filter(Embarked != "") %>% 
  # We remove the unique passenger ID from the X matrix, along with the Survived
  # label (will be used for y), and also the following variables because on
  # their own they are not good features for model training (we have already
  # generated other feats from them): Name, Ticket, Cabin.
  select(-PassengerId, -Survived, -Name, -Ticket, -Cabin) %>% 
  # Interaction variables added here
  mutate(pclass_x_numfam = Pclass*num_family_members,
         age_x_fare = Age*Fare)
  

# Now with only X vars needed for model, convert to model matrix (so that
# categoricals get dummified).
train_x <- model.matrix(~.-1, train_x_prep)

train_y <- train_final %>% 
  filter(Embarked != "") %>%
  select(Survived) %>% 
  pull()

```

```{r remove_feats_less_than_one_perc}

# Remove logical/binary feats with less > 99% uniformity in responses.

train_x_dummy_df <- as_tibble(train_x)

train_x_feat_summ <- train_x_dummy_df %>% 
  # Remove these vars consideration since they are numeric vars or are
  # categorical vars with > 2 responses.
  select(-Age, -Fare, -Parch, -SibSp, -num_family_members, -total_cabins, -pclass_x_numfam, -age_x_fare, -married_femaleYes, -married_femaleNotApplicable, -adultYes, -adultUNKNOWN) %>% 
  gather(feat, val) %>% 
  group_by(feat) %>% 
  mutate(feat_n = n()) %>% 
  ungroup() %>% 
  group_by(feat, val) %>% 
  mutate(feat_val_n = n()) %>% 
  ungroup() %>% 
  unique() %>% 
  mutate(coverage = feat_val_n/feat_n)

uniform_feat <- train_x_feat_summ %>% 
  #Identify those with > 99% uniformity
  filter(coverage > .99) %>% 
  select(feat) %>% 
  unique() %>% 
  pull()

train_x_mod <- train_x_dummy_df %>% 
  select(-one_of(uniform_feat))

# Convert back to matrix.
train_x <- model.matrix(~.-1, train_x_mod)

```


> FIRST, we need to confirm there are no columns appearing in a model matrix of TRAIN that do NOT appear in TEST.

```{r}

test_final <- readRDS('../data/test_final.rds')

test_x_prep <- test_final %>%
  # We remove the unique passenger ID from the X matrix, along with the Survived
  # label (will be used for y), and also the following variables because on
  # their own they are not good features for model training (we have already
  # generated other feats from them): Name, Ticket, Cabin.
  select(-PassengerId, -Name, -Ticket, -Cabin) %>% 
  # Interaction variables added here
  mutate(pclass_x_numfam = Pclass*num_family_members,
         age_x_fare = Age*Fare)

# Now with only X vars needed for model, convert to model matrix (so that
# categoricals get dummified).
test_x <- model.matrix(~.-1, test_x_prep)

```



```{r}

train_x_cols <- tibble(col = colnames(train_x))

test_x_cols <- tibble(col = colnames(test_x))

train_x_cols %>% anti_join(test_x_cols, by = 'col')

test_x_cols %>% anti_join(train_x_cols, by = 'col')

```


```{r model_tuning}

set.seed(1)
cvglmnet_results <- cv.glmnet(train_x,
                              train_y,
                              # Logistic model
                              family = "binomial",
                              # 5-fold cross-validation
                              nfolds = 10,
                              # Use alpha = 1 for lasso.
                              alpha = 1,
                              # Use "class" in maximizing accuracy (this is the
                              # official metric for the kaggle competition).
                              type.measure = "class")

```


```{r best_fit}

# Identify lambda for the best performing model
best_lambda <- cvglmnet_results$lambda.min

# Build the best-performing model using lambda value above.
best_fit <- glmnet(train_x,
                   train_y,
                   # Logistic model
                   family = "binomial",
                   # Use alpha = 1 for lasso.
                   alpha = 1,
                   lambda = best_lambda)

```

```{r save_cvglmnet_results}

saveRDS(cvglmnet_results, '../data/cvglmnet_results.rds')
saveRDS(best_fit, '../data/lasso_best_fit.rds')

```

